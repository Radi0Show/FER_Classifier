{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from transformers import ViTModel, ViTFeatureExtractor\n",
        "\n",
        "# Define the MLP Classifier with configurable dropout\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, hidden_units, dropout_rate):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_units)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(hidden_units, int(hidden_units / 2))\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout_rate / 2)\n",
        "        self.fc3 = nn.Linear(int(hidden_units / 2), num_classes)\n",
        "        self.output = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return self.output(x)\n",
        "\n",
        "# Function to extract features using ViT\n",
        "def extract_features(data_loader, model, device):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls in data_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            lbls = lbls.to(device)\n",
        "            outputs = model(imgs).last_hidden_state[:, 0, :]\n",
        "            features.append(outputs)\n",
        "            labels.append(lbls)\n",
        "    return torch.cat(features), torch.cat(labels)\n",
        "\n",
        "# Load a pretrained Vision Transformer model\n",
        "model_name = \"google/vit-base-patch16-224\"\n",
        "model = ViTModel.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Image transformations\n",
        "transform = Compose([\n",
        "    Resize((224, 224)),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load dataset and create splits\n",
        "dataset = ImageFolder(root='images/train', transform=transform)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Hyperparameters to tune\n",
        "batch_sizes = [16, 32, 64]\n",
        "learning_rates = [0.001, 0.0005, 0.0001]\n",
        "dropout_rates = [0.5, 0.3]\n",
        "hidden_units = [512, 1024]\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "# Testing different hyperparameters\n",
        "for batch_size in batch_sizes:\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    train_features, train_labels = extract_features(train_loader, model, device)\n",
        "    test_features, test_labels = extract_features(test_loader, model, device)\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        for dropout in dropout_rates:\n",
        "            for units in hidden_units:\n",
        "                mlp = MLPClassifier(train_features.shape[1], len(dataset.classes), units, dropout).to(device)\n",
        "                optimizer = optim.Adam(mlp.parameters(), lr=lr)\n",
        "                criterion = nn.NLLLoss()\n",
        "                scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
        "\n",
        "                # Training loop for current set of hyperparameters\n",
        "                for epoch in range(10):\n",
        "                    mlp.train()\n",
        "                    total_loss = 0\n",
        "                    for i in range(0, train_features.size(0), batch_size):\n",
        "                        batch_features = train_features[i:i+batch_size].to(device)\n",
        "                        batch_labels = train_labels[i:i+batch_size].to(device)\n",
        "\n",
        "                        optimizer.zero_grad()\n",
        "                        outputs = mlp(batch_features)\n",
        "                        loss = criterion(outputs, batch_labels)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        total_loss += loss.item()\n",
        "\n",
        "                    scheduler.step()\n",
        "\n",
        "                    # Evaluate on test set\n",
        "                    mlp.eval()\n",
        "                    correct = 0\n",
        "                    total = 0\n",
        "                    with torch.no_grad():\n",
        "                        for i in range(0, test_features.size(0), batch_size):\n",
        "                            batch_features = test_features[i:i+batch_size].to(device)\n",
        "                            batch_labels = test_labels[i:i+batch_size].to(device)\n",
        "                            outputs = mlp(batch_features)\n",
        "                            _, predicted = torch.max(outputs.data, 1)\n",
        "                            total += batch_labels.size(0)\n",
        "                            correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "                    accuracy = 100 * correct / total\n",
        "                    if accuracy > best_accuracy:\n",
        "                        best_accuracy = accuracy\n",
        "                        best_params = {\n",
        "                            'batch_size': batch_size,\n",
        "                            'learning_rate': lr,\n",
        "                            'dropout_rate': dropout,\n",
        "                            'hidden_units': units\n",
        "                        }\n",
        "                        torch.save(mlp.state_dict(), 'mlp_classifier_best.pth')\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_params} with Accuracy: {best_accuracy}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8IlLFRlypbt",
        "outputId": "76f729a6-ebae-492e-be08-e6ef46b60eec"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'batch_size': 32, 'learning_rate': 0.0001, 'dropout_rate': 0.3, 'hidden_units': 1024} with Accuracy: 61.769297484822204%\n"
          ]
        }
      ]
    }
  ]
}